{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outputs a **.csv** file with additional metrics for the organizations in the dataset. The metrics are:\n",
    "\n",
    "- issues_count\n",
    "- closed_issues_count\n",
    "- open_issues_count\n",
    "- total_issues_count\n",
    "- closed_pulls_count\n",
    "- open_pulls_count\n",
    "- total_pulls_count\n",
    "- merged_pulls_count\n",
    "- commits_count\n",
    "- first_commit_date\n",
    "- last_commit_date\n",
    "- branches_count\n",
    "- topics\n",
    "- contributors_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install PyGithub python-dotenv pandas tqdm cachetools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from github import Github\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from cachetools import cached, TTLCache\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "g = Github(getenv('GITHUB_TOKEN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = TTLCache(maxsize=1024, ttl=300)\n",
    "\n",
    "@cached(cache)\n",
    "def fetch_repo_data(repo):\n",
    "    all_issues = list(repo.get_issues(state=\"all\"))\n",
    "    open_issues_count = sum(1 for issue in all_issues if issue.state == \"open\")\n",
    "    closed_issues_count = len(all_issues) - open_issues_count\n",
    "    \n",
    "    all_pulls = list(repo.get_pulls(state='all'))\n",
    "    open_pulls_count = sum(1 for pull in all_pulls if pull.state == \"open\")\n",
    "    closed_pulls_count = len(all_pulls) - open_pulls_count\n",
    "    merged_pulls_count = sum(1 for pull in all_pulls if pull.merged)\n",
    "    \n",
    "    commits = list(repo.get_commits())\n",
    "    commits_count = len(commits)\n",
    "    first_commit_date = commits[-1].commit.author.date if commits else None\n",
    "    last_commit_date = commits[0].commit.author.date if commits else None\n",
    "\n",
    "    return {\n",
    "        \"issues_count\": len(all_issues),\n",
    "        \"closed_issues_count\": closed_issues_count,\n",
    "        \"open_issues_count\": open_issues_count,\n",
    "        \"total_issues_count\": closed_issues_count + open_issues_count,\n",
    "        \"closed_pulls_count\": closed_pulls_count,\n",
    "        \"open_pulls_count\": open_pulls_count,\n",
    "        \"total_pulls_count\": closed_pulls_count + open_pulls_count,\n",
    "        \"merged_pulls_count\": merged_pulls_count,\n",
    "        \"commits_count\": commits_count,\n",
    "        \"first_commit_date\": first_commit_date,\n",
    "        \"last_commit_date\": last_commit_date,\n",
    "        \"branches_count\": int(repo.get_branches().totalCount),\n",
    "        \"topics\": str(tuple(repo.topics)),\n",
    "        \"contributors_count\": int(repo.get_contributors().totalCount)\n",
    "    }\n",
    "\n",
    "def get_repo_data(full_name):\n",
    "    repo = g.get_repo(full_name)\n",
    "    return fetch_repo_data(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_all_repo_data(full_names):\n",
    "    data_list = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        future_to_repo = {executor.submit(get_repo_data, full_name): full_name for full_name in full_names}\n",
    "        for future in tqdm(as_completed(future_to_repo), total=len(full_names), unit=\"repo\", ncols=100, bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}] {percentage:3.0f}%\"):\n",
    "            try:\n",
    "                data = future.result()\n",
    "                full_name = future_to_repo[future]\n",
    "                data[\"full_name\"] = full_name\n",
    "                data_list.append(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching data for repo: {future_to_repo[future]}, error: {e}\")\n",
    "\n",
    "    return pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('codesamples.csv')\n",
    "full_names = dataframe['full_name'].unique()\n",
    "\n",
    "repo_data_df = fetch_all_repo_data(full_names)\n",
    "\n",
    "dataframe = pd.merge(dataframe, repo_data_df, on='full_name', how='left')\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "file_path = f'results/codesamples_full_{timestamp}.csv'\n",
    "dataframe.to_csv(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
